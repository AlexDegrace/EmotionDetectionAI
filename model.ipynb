{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13000), started 0:16:39 ago. (Use '!kill 13000' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-75fa84d038102c28\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-75fa84d038102c28\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Alex DeGrace 300071786\n",
    "Gabriel St-Pierre 300146514\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train percentage of angry image: 13.92%\n",
      "train percentage of disgusted image: 1.52%\n",
      "train percentage of fearful image: 14.27%\n",
      "train percentage of happy image: 25.13%\n",
      "train percentage of neutral image: 17.29%\n",
      "train percentage of sad image: 16.82%\n",
      "train percentage of surprised image: 11.05%\n",
      "Total number of train data: 28709\n",
      "----------------------------------------------\n",
      "test percentage of angry image: 13.35%\n",
      "test percentage of disgusted image: 1.55%\n",
      "test percentage of fearful image: 14.27%\n",
      "test percentage of happy image: 24.71%\n",
      "test percentage of neutral image: 17.18%\n",
      "test percentage of sad image: 17.37%\n",
      "test percentage of surprised image: 11.58%\n",
      "Total number of test data: 7178\n",
      "----------------------------------------------\n",
      "Percentage of image for training: 80.00%\n",
      "Percentage of image for test: 20.00%\n"
     ]
    }
   ],
   "source": [
    "# Dont't forget to get the data from Kaggle (https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer) and make sure that they are in the data folder.\n",
    "# The data folder should look something like this: ./data/train/ and ./data/test/.\n",
    "# Both folder should have these different subfolder full of png:\"\" angry, discusted, fearful, happy, neutral, sad and surprised.\n",
    "\n",
    "# Array containing all possible class\n",
    "classes = [\"angry\",\"disgusted\",\"fearful\",\"happy\",\"neutral\",\"sad\",\"surprised\"]\n",
    "# Array containing train and test for looping\n",
    "folders_name = [\"train\",\"test\"]\n",
    "dir_path = './data'\n",
    "\n",
    "folder_img_count_array = []\n",
    "# Loop trough the folders_name array so that we visit both folders\n",
    "for folder_name in folders_name:\n",
    "    folder_img_count = 0\n",
    "    class_distribution_array = []\n",
    "    #Loop trough the class dictionary so that we go inside each class folders\n",
    "    for c in classes:\n",
    "        current_dir_path = dir_path + \"/\" + folder_name + \"/\" + c\n",
    "        count = 0\n",
    "        # Iterate directory\n",
    "        for file_path in os.listdir(current_dir_path):\n",
    "            # check if current path is a file\n",
    "            if os.path.isfile(os.path.join(current_dir_path, file_path)):\n",
    "                count += 1\n",
    "                folder_img_count += 1\n",
    "        class_distribution_array.append(count)\n",
    "    folder_img_count_array.append(folder_img_count)\n",
    "    #Print the class distribution of each dataset (train/test)\n",
    "    print(f\"{folder_name} percentage of angry image: {class_distribution_array[0]/folder_img_count*100:.2f}%\")\n",
    "    print(f\"{folder_name} percentage of disgusted image: {class_distribution_array[1]/folder_img_count*100:.2f}%\")\n",
    "    print(f\"{folder_name} percentage of fearful image: {class_distribution_array[2]/folder_img_count*100:.2f}%\")\n",
    "    print(f\"{folder_name} percentage of happy image: {class_distribution_array[3]/folder_img_count*100:.2f}%\")\n",
    "    print(f\"{folder_name} percentage of neutral image: {class_distribution_array[4]/folder_img_count*100:.2f}%\")\n",
    "    print(f\"{folder_name} percentage of sad image: {class_distribution_array[5]/folder_img_count*100:.2f}%\")\n",
    "    print(f\"{folder_name} percentage of surprised image: {class_distribution_array[6]/folder_img_count*100:.2f}%\")\n",
    "    print(\"Total number of\",folder_name,\"data:\",folder_img_count)\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "total_num_image = folder_img_count_array[0] + folder_img_count_array[1]\n",
    "#The total number of training images is 80% but 25% of the 80% (20% total) will be use for validation\n",
    "print(f\"Percentage of image for training: {folder_img_count_array[0]/total_num_image*100:.2f}%\")\n",
    "print(f\"Percentage of image for test: {folder_img_count_array[1]/total_num_image*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train\n",
      "Found 21535 images belonging to 7 classes.\n",
      "Found 7174 images belonging to 7 classes.\n",
      "Printing the first image in the train_generator\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAerUlEQVR4nO2dbYwf1XXGn4MxLzG2wcYYYxsMsdMEoRZLFgpJPyAoEhAU+BBVIVFFJSS+tBJRUiWklapGaiXyJS9Sq1SoRHGlCPIqgVCqigIRsVQ5McFQDLHZWATWb0uMDYYQ8+LbD/tf5Hnm2Z2zs7v/XXOfn4TsO75z586dOcyeZ885N0opMMZ88DltvidgjBkONnZjKsHGbkwl2NiNqQQbuzGVYGM3phJmZOwRcUNE7I6IkYi4e7YmZYyZfaLv79kjYhGAPQCuBzAK4FcAbiulPDfZOStXrizr16/ncaZsA0BmjnzeiRMnpn2OupYaR53XNY46LzPOXJK5V3Ufp512WmefPqj1mK0163uvDN97duz33nuvc5x33nlnynMA4Iwzzmi0Fy1a1Gjv378fR44ckYt0ujqY5CoAI6WUvQAQEQ8AuAXApMa+fv16PPbYY41jfNOnn96eUsZwFy9e3Gi/9dZbrT78UPgcoL3gf/jDHzqvpeb39ttvt46dddZZjbZ64Or+mcxLyS8KvxRA+17feOONVh91b/zCvfvuu53zUffKY5955pmd56lnxvem/ofwxz/+cco20F4PxYc+9KHO89Q6Hj16tHOcgwcPTnkOAFx22WWN9jnnnNNo33bbba1zJpjJj/FrAbx8Unt0cMwYswCZc4EuIu6MiB0RsePw4cNzfTljzCTMxNj3ATjZAV83ONaglHJvKWVLKWXLypUrZ3A5Y8xMmInP/isAmyLiUowb+WcBfG5WZkUoP41hH1X5tRk/NgP7aMrXU/4nXz+jRag5Zu6D/WjVh9dV+dXKt+W1VT4yj6X8eu6jBKnMmvFaK92Dx1HvlPK12bfOvDNKr+Fxli5d2urDPjprI0DunZmM3sZeSnk3Iv4WwH8DWATgu6WUXb1nYoyZU2byZUcp5WcAfjZLczHGzCGOoDOmEmb0ZZ8up512mvRDTkb5dhk/iX0Z5f9xH+Wj8nlqHOWTMZnf4as+7A9nfoedQekKfH01H3WvvI4cP6DIaCiZQJNMUE3mfVHxE8ofPvvsszvH4pgOtR7ss6vnyse6bAVo6xVTrY+/7MZUgo3dmEqwsRtTCTZ2YyphqAKdgsUUJVxkhC0WdzKJH5kABdWH55jNoOI5KfGvD2qczLX6BmjMVdZb5tn3FSxZkFNCVibCMxNkpAQ6FtKUYMrBQOeee26rD4t2nAgzlTjpL7sxlWBjN6YSbOzGVMLQffaugIdMAkmfcYG2n5ZJvFC+FfuNyo9TASIq+YHpoyOo++B1VOvDc8wUb1DX79vn+PHjjbZKYOGgHhXkw4Evx44d6xxn2bJlrT6ZoJ6MXpQpsKGeGfv6aj0y2tRk+MtuTCXY2I2pBBu7MZVgYzemEoYq0EVES2DIiGazFSCSqYzC4puqUstij6oCqubDgR0rVqxo9WFxJyNOZoJR+gZ6ZIJYlKjKgSaZZ5bJsFMiGj8jJZiy2JWtVMNwEIuaU6ayUqZqb+ad5nOc9WaMsbEbUws2dmMqYag+eymlcxeQTKCLIlPxtescoB3ooSqasE/4+uuvt/qowI5MUI3aKaQPmWQdXlc150xV2Iyvn9npR/nsfC11H6yZqGuxr5159oDWI5g+CUXKr+dx1H3wsekkIfnLbkwl2NiNqQQbuzGVYGM3phLmvVINkyndnCnvqwIrMkELLLapQAs+pkQ8dV5GNORsLBUMkxGtMgIdH1Nrlil5rMgIdPxcM9fP3Ov555/f6sNr9vvf/77VR8Hnvfnmm519Mlt2KTFQBQwxmfLSk+EvuzGVYGM3phJs7MZUwtB99q5KMMof5MAB5ftmtvbN+ER8fRUww0Ec6lrKJ8tsG8x+rAqyYT9e3RffhwpYyfRR2oPyv7vG7uuz8zF1rxwwo96hsbGxzmspf5jvXyUv8fWV7sTJOmqt+blm+rA+4EQYY4yN3ZhasLEbUwk2dmMqYd6DajKVarjPbG0RpcQeFkAyQTUqiCKTLZapjKLGyVRvUYE+DItUSmjMVH1Rc8wEMGUEOhY61XPl+bAYB7SFTiV+ZUo3qyCnTJAXr5HKgOR3WM0nk4U3Gf6yG1MJNnZjKqHT2CPiuxExFhHPnnRsRUQ8EhEvDP48b26naYyZKRmf/XsA/hXAf5507G4Aj5ZS7omIuwftr2QuyP4t+04qYCYTbJDxYxkV+MLXUtvmMso/Vv4W+5vK/+I+mfvIVIFRc+R1VNfK+LaqT8Zn52OZ4KRMRWB1H6yPqHEyySkqyElVnGX4PBXAc/DgwUZb6QN8jNdsqso1nW9SKeUJAK/S4VsAbB38fSuAW7vGMcbML3199tWllAODvx8EsHqW5mOMmSNmLNCV8Z8bJv3ZISLujIgdEbEjm0NsjJl9+hr7oYhYAwCDP9u/2BxQSrm3lLKllLJFFRUwxgyHvkE1DwG4HcA9gz8f7DuBTNBEpsoHC3tK/MrsZc3nfexjH2v14ZLLKqhEiVZMpupIpiqMgkU09VNVJvAmI75lqtlMp+TxyWTKTfNzzaxrtpIQZ6uprDee03nntX85xe913/LbmXEmI/Ort/sB/C+AP4mI0Yi4A+NGfn1EvADgLwZtY8wCpvPLXkq5bZJ/um6W52KMmUMcQWdMJQx9+6euIAnlb3GChAqIYF9G+f6ZpJuMhpBJIFHBF3xMBU2wD5aptptJllHi6KuvNsMnssFBfXSEzBZJaj242q6aD/fJbKusfH9eD6BdlSgTeJTZwkutNd+/uo8+W01N4C+7MZVgYzemEmzsxlSCjd2YShiqQHfixImWMMFBLEqAmE7gwARKAOGxlSDDpaNZ/AHaIs1LL73U6pPZI1wJMH32Z8+INpmqPEq04qASoC1iqrF5Tkp4ZfEzUzZbPY/MumYCgVSGI6+Jeq48b/W+Zva053tTgWG8rkrAnQx/2Y2pBBu7MZVgYzemEmzsxlTC0EtJs1DRp0y0IrNnemZfcxZ3VMlfPnbhhRd2zg/IlZxiQaxvOSkWkjKlnJVApzIMOYIwkxm3fPnyVh8W35T4lenD74wSXnkdM/vKKVQkohINGZ5jJsJSrX2XoD2jslTGmA8GNnZjKsHGbkwlDNVnj4iW38o+RiaDK1NOWMG+tvLH2f9SwSCZ7Y+U78Q+GFe8Ado+obrX1157rdFWVVcygTZ8XsbXVecpXzfja69du7bR3rBhQ6sP6yGZoCOl8fB9vPLKK60+o6OjrWP79++fsg20NRPl1/M7MpNtnE5mOllw/rIbUwk2dmMqwcZuTCXY2I2phHnfnz2zl1cms4f3yFZCDmc1qT6Zkk8ceKPEJxUQ8eyzzzbaKltu8+bNjbYSETkzj9tA+z6U0MbCmuqjSlDz9dR5vLaXXXZZq88VV1zRaF9yySWtPn2ELCVYsqi5cePGVh91/d27dzfae/bsafXhe1XvAwu26p3mkthK5OXzuO2gGmOMjd2YWrCxG1MJQ/fZu3wM5eu++eabjXYm8UP5bexrqoAZ9hE5UQdo+6jKrzxy5EjrGO+/ra7PgTZqPdiP3rt3b6tPZgukTCKMWkfWGlQwzkUXXdRoq+oxrLOoYBg+pspNMyrw5Te/+U2jrTSEiy++uHVs/fr1jbbScNatW9doq/eT3yO1FRnbhloPVdo8i7/sxlSCjd2YSrCxG1MJNnZjKmHoe72x6MBZOyqDikWizH7gSqRhVOlgznpTWUUsBioxTB3jvb2VIMbil7o+r5ES8XjN+u5Zp9aahc59+/Z1nsd7pgHArl27Gm0VjJIJhOL7UIEv/DzGxsZafV588cXWMX4f1P7sLBoq8Y2FaNUng0tJG2M6sbEbUwk2dmMqYeg+O/sc7G9mqn6qxA/uo6rAsH+jAkbY/8roAypgRAV/8HkqaIKDL5Tvz/NW98oofSBT3UfpAZmKpocOHWq0OahFjaN0BQ68UeuaqRrMfUZGRlp9du7c2TrGmsHNN9/c6sPzziS5ZFDPnt8ZfoZOhDHG2NiNqQUbuzGV0GnsEbE+Ih6PiOciYldE3DU4viIiHomIFwZ/njf30zXG9CUj0L0L4EullF9HxFIAT0bEIwD+GsCjpZR7IuJuAHcD+Mp0J8BihhKNMlVXuI/aV5yFGyVasZCTEbZUMIiCx1Zi05IlSxptJfZwllkm8EeRCdBQAl0GzlR8+umnW31YVF25cmWrD69HZi94JfIePny40VZClqrKw8FAStRVc2L4PVLX50xJlU3JYiDbwowEulLKgVLKrwd/PwbgeQBrAdwCYOug21YAt3aNZYyZP6bls0fEBgCbAWwHsLqUcmDwTwcBrJ7knDsjYkdE7FB5z8aY4ZA29og4B8BPAHyhlNL4RXcZ/9lB/vxQSrm3lLKllLJFxRUbY4ZDytmMiMUYN/Tvl1J+Ojh8KCLWlFIORMQaAO3Mgh5kfHblk/F5ytdWx5iMX8++rfJrM4EVmSSTzDbCyj9nPzITHKT8vczYSnvgQJ8DBw60+vD1VFWgD3/4w412JshIrT1rOOqZqWfNz0Ml9PC81fWn8qUn4HVVa8YfTH6n1Bq+P37XBGL87PsAPF9K+cZJ//QQgNsHf78dwINdYxlj5o/Ml/2TAP4KwP9FxM7Bsb8HcA+AH0bEHQB+B+Av52SGxphZodPYSynbAEz2s8F1szsdY8xc4Qg6Yyph3ktJd2XxAG3hJFO9RYktHOihRJNMJhhfP7OnvEKJPTynTAaXyrrLBHrws8gEGSkylXpUZh4Le5ky4qrcM6MCXzLZlUpY4/XnbL4sPLbKguMqPKoqD//6Wu0FPxn+shtTCTZ2YyrBxm5MJQzVZz9x4kSn/52pVKP82Ixf3zWuOq/vtdQxlZzDZLb3yWztxCjfm/3G7H1k+vDYyh/OPFdOTlFryBVg1RqyNpQJzALavj4n1AC5IKuu+SiWL18+7XFcqcYYY2M3phZs7MZUgo3dmEoYelANCwh9stUy2WsKFo2U0JW5VkbEU8cywg2LPWqOHIyiBCkW5FT2WiY4KBNkpMQ/vg/Vh8fOBBBxkA3QFu24/LRCCWSZ56iEPX6v1NiZoJqMGMqiXUb0ncBfdmMqwcZuTCXY2I2pBBu7MZUwdIGOYZEmIwhlUIIQi1QZ8U3Nh0UaVdpancfCjSoVzHNSmU+c5aYEOr7XviWYlQDEQqNaRxZiM2udEREVvNYqw47vP5PNp/plSnSraEHOxDvvvPY2C/x+qLXPli1X+MtuTCXY2I2pBBu7MZUw7z57plQy+6R99roGcsEw7DdlgnyyQTXKR2c4YIb3BwfaPrsq5Zwh4w9nSmlnyGR5KX1CVZ3pQ2bLLvV8uJ+qCsTroco589iZwJuDBw+2+qxeLfdiSeEvuzGVYGM3phJs7MZUgo3dmEoYukA31V5UgA5G6TpHobLFMiINi3+Zvc5UgIYSzXgsJZCxSJUR6FSWFwdkZMQ4dR9q7TP7lvG9qow/nuOaNWtafTIln/pkzynUffH7oDYnzZTO4vVQAl1mL0AuZe1S0saYFjZ2YyrBxm5MJcx7UE3G3+rjIyp/h/025Y9mEkj4vIw+kD2PyyJnEmEUmWo+fZJugHaAiPKjMwkbmYCqTDBKNqml6xyVULRkyZJG+4ILLmj1OXLkSKOtAoH4fVi3bl1nH6UP8NichDWVNuMvuzGVYGM3phJs7MZUgo3dmEpYcKWkM8Efij5BI0qQ4SAOJVoxSgzLBAJlBDrVh4NoWCACgNdff73RVkJbJlvuwgsvbB1jQS6TQabugwU6lU3H659ZazVOJlhKPTPOMlNBLPv372+01bp2CWtA+/1UAiavB49rgc4YY2M3phY6jT0izoqIX0bE0xGxKyK+Njh+aURsj4iRiPhBREx/H2FjzNDI+OzHAVxbSnkjIhYD2BYR/wXgiwC+WUp5ICL+HcAdAL7TNRj7UxlfO1OZJlMFtE/whSJTdUVdi4+pJBf29zKVYjJbRKk+HDCi1lkFdrD/Pzo62urz2muvNdpq2yb2/ftU8gHa66p8dvbHs+/Chg0bGm31zHbu3NloX3TRRZ1zfPXVV1t92B9XPntXlaQZ7c9explQARYP/isArgXw48HxrQBu7RrLGDN/pP73FhGLImIngDEAjwD4LYCjpZQJKX0UwNo5maExZlZIGXsp5b1SypUA1gG4CsBHsxeIiDsjYkdE7FA/uhhjhsO0nNhSylEAjwO4GsC5ETHhGK8DsG+Sc+4tpWwppWxR/p8xZjh0CnQRsQrAO6WUoxFxNoDrAXwd40b/GQAPALgdwINdY5VSegXN8DlKSMpUj8mIeBzko66V2f9bwddTATuZ/eE5YObiiy9u9eGsKhUMwuvK4052fQ60UYLU0aNHG22VCcYVXjg4BchlxmUy7PhelWCp3s2NGzc22kogZKFTBczweij43cts2cXPZyqBLqPGrwGwNSIWYfwngR+WUh6OiOcAPBAR/wzgKQD3JcYyxswTncZeSnkGwGZxfC/G/XdjzCmAI+iMqYShJ8Kwz9F3K12Gk1qU/5XZtolRPlDfSq3soytfU1V9YXhbIJXQs2nTpkZbzZn9VuV7s1+tzuPkHQAYGxtrtLdt29bq89xzz7WO9SGz/RJrL9ntudhnV2Qq9/A6qvfq0ksvbbSV78+Vcvg3XFNtee4vuzGVYGM3phJs7MZUgo3dmEoYqkCXCarJVDRR4ldmz/DM9kuM6jOVCDKBug8W6NQ4maxAFndefvnlVp/Dhw832ipbi9dDCXRKfON5v/jii60+TzzxRKOtxLhMxRu+lhIjeT1UIFQm623VqlWtY7wmSjTjYyoQifuoZ79y5crOPmwLmXdxAn/ZjakEG7sxlWBjN6YS5n37J04sUIkPfbZkyvh/mQSKTMKE8tGU38jXV+fx2H2Dg/hamWqmKhFFbaPMa71jx45Wnz179jTaKtCEfW1VASiT6KH8eCZTBWft2nZJhqVLlzbax44d6xxHwe+aSjpinUXdF5/Hz3Uq7cpfdmMqwcZuTCXY2I2pBBu7MZUwdIGuKwggE4wyl3Rl5aljas5KKMkE/mTg6yuBjo+pTDB+FqqazMjISOsY34fafoqFtb4ZfnwfmSASJY7yM1J9OOsMaAtySujksZXIzPeqKtfwuqry21zGm0XVqcqc+8tuTCXY2I2pBBu7MZUwdJ89EyDDZKrCZgIr5orZ3GZaBdowGd+fg0+Uf8wBGi+99FKrj6pUk0koUr5tF2ocvlbm2as+7KNz0slk52USWPh5ZIKs1FrzVlPKNtjX5/nMaPsnY8wHAxu7MZVgYzemEmzsxlTCUAW6iOgU6NT2OplstWEKdJmglr4lsfleMxltShDi4Aq1Zhygwds6TQaXL1ZrzxVeuPw10L43lZnGY6v3g1F9OHtNbYfFmXoA8JGPfKTRVgEzvBe92sCU56QCxXiNeAsvoC0YciDUVGKxv+zGVIKN3ZhKsLEbUwk2dmMqYegCHQsunDGUiaJS4hePo/rwtVUfFrtUdlSmnFWmj4p24mgsFbHFa6T2i8/sIZ+J1lORcAcOHGi033rrrc5xVJbX888/32iryMDly5c32irDkMVIJRiyQKfuPSPavfLKK51zVNFxLH6q++CIxkw2I0c4eq83Y4yN3ZhasLEbUwnzXko6k9WU8etVsAPD/kymokkmgykzZ3VMncfH1DiZAA3WIzK+v0Kdl9nuiIM91L1efvnljbYKasnMh4NYVFANB+yoUs5qL3auBMP7zivU2Jm15metgnN4G6/pBG/5y25MJdjYjamEtLFHxKKIeCoiHh60L42I7RExEhE/iIj2z5vGmAXDdL7sdwE4+RejXwfwzVLKRgBHANwxmxMzxswuKYEuItYB+BSAfwHwxRivS3wtgM8NumwF8E8AvjPdCbDAkAmYUSKNErIYFkmUQMeBHZlS0koQ4owyoB20ofZDz+xJlimDlMmM48CbzF5rQPt+VZ8VK1Y02mo9OKPsmmuuafXhfd23bdvW6sPin7pWJmBFlW5mkUwJwRwwpAKReI1UHxZaVSASv3scQKRKhr9/7qT/0uRbAL4MYGKFVgI4WkqZeKNGAbR3xTPGLBg6jT0ibgYwVkp5ss8FIuLOiNgRETvUZgLGmOGQ+TH+kwA+HRE3ATgLwDIA3wZwbkScPvi6rwOwT51cSrkXwL0AcMUVV0xe+tIYM6d0Gnsp5asAvgoAEXENgL8rpXw+In4E4DMAHgBwO4AHMxfs2kc9k0CS2dc8Q+acTOCLCmpR53Ggyd69ezvPyyTrZAJNVElo1iyUz6qCSHgs9Tz4PlavXt3qwwkkF1xwQauP8lsZ9tFVMAr7yKoqz6FDh1rH+F6VrsEahgqq4WAY5bPz9Xl9FPwOz1Up6a9gXKwbwbgPf98MxjLGzDHTCpctpfwcwM8Hf98L4KrZn5IxZi5wBJ0xlWBjN6YShp71xgICi0SZ4Ji++7VnMoQy+4FnxlGiFQs3amxeH1V1JSNqZoROFoSywqcKWmFYjHzyyfZvbvnZs4gFAKtWrWq0ly1b1uozOjraaHMFHADYvHlzo63uizPcVL9MxqUKvOEKN+q58vorcZKFPX7OU+0D6C+7MZVgYzemEmzsxlTCvFeqYfpu45QJkGGfSOkDffxxdY7y2/h6metn5pPZ2kn52ewjKx9R3UemCg7rE+q5Zqr98r0pXWHXrl2d1+LAF7UdFesMQFtXUNdnDSmzZZfyrfl9UFV72Wfne7XPboyxsRtTCzZ2YyrBxm5MJQxdoGMBQVWLYTKiXSb4YbZgsUcJVH23iMqIeHz9TCCSgivnZCreKJSwxccOHz7c6sPHjh071urDNRB+8YtftPpwZtrVV1/d6sPioxK/lBjJa62y3nhs9e7xe6+ENA6oUn34+tz2/uzGGBu7MbVgYzemEobqs5dSOn3rjH+uqrn2CcZRQRzso2a2zc1UxFUon53HygRo9A0O4mCQ7Dh8ffU8uI96Pnx9pRnwFlEKDpBR21Xv3r270ebqt4CuMLNkyZJGu6/Pzqh15SpOaj2OHz/eaGe0gPev2TkrY8wHAhu7MZVgYzemEmzsxlTCUAW6iEhlTDFTiQ4TcHDO0qVLW31YEFNBFHxstspWq/P67pnOmU+qTDST3UOeUQEzLEip+2Ahi4UloL3Wn/jEJ1p9rrvuukZblWDm+9+3r72FwcjISKO9ffv2Vh9V2pvLa6tSzXxvmYAqtU0Tr0dGnOXgIAfVGGNs7MbUgo3dmEqY90o1meCPTLJMxq/nII7MdsyKjO+dOZYJ2Mn49ere2Y/MXEvpE6qiC28JpZ5PZuvpK6+8stFm/xxo+6TqXnmbJHXtTZs2Ndo33nhjq486dv/99zfazzzzTKsPo/xxtbZ9+mTeocnwl92YSrCxG1MJNnZjKsHGbkwlxFT7Oc/6xSJeAfA7AOcD6I4EWVicinMGTs15e879uaSUskr9w1CN/f2LRuwopWwZ+oVnwKk4Z+DUnLfnPDf4x3hjKsHGbkwlzJex3ztP150Jp+KcgVNz3p7zHDAvPrsxZvj4x3hjKmHoxh4RN0TE7ogYiYi7h339DBHx3YgYi4hnTzq2IiIeiYgXBn+eN59zZCJifUQ8HhHPRcSuiLhrcHzBzjsizoqIX0bE04M5f21w/NKI2D54R34QEf12wZhDImJRRDwVEQ8P2gt+zkM19ohYBODfANwI4HIAt0VEd/nQ4fM9ADfQsbsBPFpK2QTg0UF7IfEugC+VUi4H8HEAfzNY24U87+MAri2l/BmAKwHcEBEfB/B1AN8spWwEcATAHfM3xUm5C8DzJ7UX/JyH/WW/CsBIKWVvKeVtAA8AuGXIc+iklPIEgFfp8C0Atg7+vhXArcOcUxellAOllF8P/n4M4y/iWizgeZdxJkrhLB78VwBcC+DHg+MLas4AEBHrAHwKwH8M2oEFPmdg+Ma+FsDLJ7VHB8dOBVaXUg4M/n4QwOr5nMxURMQGAJsBbMcCn/fgx+GdAMYAPALgtwCOllImcjkX4jvyLQBfBjCRX7oSC3/OFuj6UMZ/hbEgf40REecA+AmAL5RSGrseLMR5l1LeK6VcCWAdxn/y++j8zmhqIuJmAGOllCfney7TZdjFK/YBWH9Se93g2KnAoYhYU0o5EBFrMP4lWlBExGKMG/r3Syk/HRxe8PMGgFLK0Yh4HMDVAM6NiNMHX8qF9o58EsCnI+ImAGcBWAbg21jYcwYw/C/7rwBsGiiXZwD4LICHhjyHvjwE4PbB328H8OA8zqXFwG+8D8DzpZRvnPRPC3beEbEqIs4d/P1sANdjXGt4HMBnBt0W1JxLKV8tpawrpWzA+Pv7WCnl81jAc36fUspQ/wNwE4A9GPfN/mHY10/O8X4ABwC8g3H/6w6M+2WPAngBwP8AWDHf86Q5/znGf0R/BsDOwX83LeR5A/hTAE8N5vwsgH8cHL8MwC8BjAD4EYAz53uuk8z/GgAPnypzdgSdMZVggc6YSrCxG1MJNnZjKsHGbkwl2NiNqQQbuzGVYGM3phJs7MZUwv8DjbmeU69AGb4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Specify the batch size\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "#Create the training ImageGenerator to feed our model more and different data so that can perform better \n",
    "image_generator_training = ImageDataGenerator(\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.5,1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=1/255,\n",
    "    validation_split=0.25 # takes 25% of test data for validation (20% of total data)\n",
    ")\n",
    "\n",
    "#Trainning directory path\n",
    "train_dir = dir_path + \"/\" + folders_name[0]\n",
    "print(train_dir)\n",
    "\n",
    "# Saving the augmented data was causing errors.\n",
    "\n",
    "train_generator = image_generator_training.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    #classes=classes,\n",
    "    class_mode='sparse',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    # save_to_dir=\"./augemented_data\",\n",
    "    # save_prefix='agm_img',\n",
    "    # save_format='png',\n",
    "    subset=\"training\",\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "val_generator = image_generator_training.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    #classes=classes,\n",
    "    class_mode='sparse',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    # save_to_dir=\"./augemented_data\",\n",
    "    # save_prefix='agm_img',\n",
    "    # save_format='png',\n",
    "    subset=\"validation\",\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Printing the first image in the train_generator\n",
    "print(\"Printing the first image in the train_generator\")\n",
    "for i, element in enumerate(train_generator):\n",
    "    plt.imshow(element[0][1], cmap=plt.cm.binary)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 48, 48, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 45, 45, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 23, 23, 32)        1056      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16928)             0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 10)                169290    \n",
      "=================================================================\n",
      "Total params: 170,890\n",
      "Trainable params: 170,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "336/336 [==============================] - 16s 47ms/step - loss: 0.8809 - accuracy: 0.2850 - val_loss: 0.8468 - val_accuracy: 0.3237\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 2/15\n",
      "336/336 [==============================] - 14s 42ms/step - loss: 0.8290 - accuracy: 0.3495 - val_loss: 0.8179 - val_accuracy: 0.3614\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 3/15\n",
      "336/336 [==============================] - 14s 42ms/step - loss: 0.7976 - accuracy: 0.3864 - val_loss: 0.8043 - val_accuracy: 0.3810\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 4/15\n",
      "336/336 [==============================] - 15s 45ms/step - loss: 0.7766 - accuracy: 0.4032 - val_loss: 0.7937 - val_accuracy: 0.3910\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 5/15\n",
      "336/336 [==============================] - 14s 43ms/step - loss: 0.7626 - accuracy: 0.4151 - val_loss: 0.7890 - val_accuracy: 0.3962\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 6/15\n",
      "336/336 [==============================] - 15s 43ms/step - loss: 0.7496 - accuracy: 0.4281 - val_loss: 0.7642 - val_accuracy: 0.4193\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 7/15\n",
      "336/336 [==============================] - 15s 43ms/step - loss: 0.7382 - accuracy: 0.4419 - val_loss: 0.7633 - val_accuracy: 0.4238\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 8/15\n",
      "336/336 [==============================] - 14s 43ms/step - loss: 0.7316 - accuracy: 0.4448 - val_loss: 0.7532 - val_accuracy: 0.4309\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 9/15\n",
      "336/336 [==============================] - 15s 44ms/step - loss: 0.7232 - accuracy: 0.4507 - val_loss: 0.7524 - val_accuracy: 0.4367\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 10/15\n",
      "336/336 [==============================] - 14s 42ms/step - loss: 0.7155 - accuracy: 0.4593 - val_loss: 0.7429 - val_accuracy: 0.4385\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 11/15\n",
      "336/336 [==============================] - 15s 43ms/step - loss: 0.7072 - accuracy: 0.4668 - val_loss: 0.7404 - val_accuracy: 0.4415\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 12/15\n",
      "336/336 [==============================] - 15s 43ms/step - loss: 0.7020 - accuracy: 0.4717 - val_loss: 0.7456 - val_accuracy: 0.4388\n",
      "Epoch 13/15\n",
      "336/336 [==============================] - 14s 43ms/step - loss: 0.6957 - accuracy: 0.4769 - val_loss: 0.7363 - val_accuracy: 0.4438\n",
      "INFO:tensorflow:Assets written to: models\\assets\n",
      "Epoch 14/15\n",
      "336/336 [==============================] - 15s 43ms/step - loss: 0.6909 - accuracy: 0.4815 - val_loss: 0.7430 - val_accuracy: 0.4426\n",
      "Epoch 15/15\n",
      "336/336 [==============================] - 15s 45ms/step - loss: 0.6854 - accuracy: 0.4869 - val_loss: 0.7491 - val_accuracy: 0.4350\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e6b906a940>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(48,48,1))\n",
    "\n",
    "x = layers.Conv2D(filters=32, kernel_size=4, activation=\"relu\", name=\"conv2d_1\")(inputs) \n",
    "x = layers.MaxPooling2D(pool_size=2,padding=\"same\", name=\"max_pooling2d_1\")(x) \n",
    "x = layers.Dense(32, activation=\"relu\", name=\"dense1\")(x) \n",
    "x = layers.Flatten(name=\"flatten\")(x)\n",
    "output = layers.Dense(10, activation='softmax', name='dense_out')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model_metrics = ['accuracy']\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", loss_weights=0.5, metrics=model_metrics)\n",
    "\n",
    "\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"models\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\"\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=5e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=train_generator,\n",
    "    y=None,\n",
    "    batch_size=batch_size,\n",
    "    epochs=15,\n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard],\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.samples/batch_size,\n",
    "    validation_steps=val_generator.samples/batch_size,\n",
    "    validation_batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcbc43bb51c6b710079320fd135063e28012cf5b36063b1ffdde6cef63bfa50e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
